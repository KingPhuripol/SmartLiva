# üì¶ ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Model Files ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà

## ‚ö†Ô∏è ‡∏õ‡∏±‡∏ç‡∏´‡∏≤

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î **48GB** ‡πÄ‡∏û‡∏£‡∏≤‡∏∞:

- `medgemma_model.pth` - **16GB** üò±
- `data/` (training images) - **15GB**
- `maxvit_large_best.pth` - **805MB**

‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:
- ‚ùå **‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£** ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Git
- ‚ùå **‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£** deploy ‡πÑ‡∏õ Vercel (limit 100MB)
- ‚úÖ **‡∏Ñ‡∏ß‡∏£** ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Cloud Storage

---

## üßπ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î (‡πÄ‡∏£‡πá‡∏ß!)

```bash
# ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
./cleanup-large-files.sh
```

‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏•‡∏ö:
- ‚úì Model files (.pth)
- ‚úì Training data (data/)
- ‚úì node_modules (‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ)
- ‚úì .next build
- ‚úì Python cache
- ‚úì Jupyter checkpoints

‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î: **48GB ‚Üí ~100MB** üéâ

---

## üì§ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö Model Files

### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 1: Hugging Face Hub (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ - ‡∏ü‡∏£‡∏µ!)

#### 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡πÅ‡∏•‡∏∞ Upload

```bash
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
pip install huggingface_hub

# Login
huggingface-cli login

# ‡∏™‡∏£‡πâ‡∏≤‡∏á repository
huggingface-cli repo create smartliva-models --type model

# Upload models
huggingface-cli upload kingphuripol/smartliva-models \
  medgemma_model.pth \
  maxvit_large_best.pth
```

#### 2. ‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ download ‡∏à‡∏≤‡∏Å Hugging Face

‡πÅ‡∏Å‡πâ `backend/app/main.py`:

```python
from huggingface_hub import hf_hub_download
import torch

@app.on_event("startup")
async def load_models():
    # Download from Hugging Face
    maxvit_path = hf_hub_download(
        repo_id="kingphuripol/smartliva-models",
        filename="maxvit_large_best.pth",
        cache_dir="/tmp"
    )
    
    medgemma_path = hf_hub_download(
        repo_id="kingphuripol/smartliva-models",
        filename="medgemma_model.pth",
        cache_dir="/tmp"
    )
    
    # Load models
    maxvit_model = torch.load(maxvit_path)
    medgemma_model = torch.load(medgemma_path)
```

### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 2: AWS S3

#### 1. Upload to S3

```bash
# Install AWS CLI
brew install awscli

# Configure
aws configure

# Create bucket
aws s3 mb s3://smartliva-models

# Upload models
aws s3 cp medgemma_model.pth s3://smartliva-models/
aws s3 cp maxvit_large_best.pth s3://smartliva-models/

# Make files public (optional)
aws s3api put-object-acl --bucket smartliva-models \
  --key medgemma_model.pth --acl public-read
```

#### 2. ‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ download ‡∏à‡∏≤‡∏Å S3

```python
import boto3
import os

@app.on_event("startup")
async def load_models():
    s3 = boto3.client('s3')
    
    # Download models to /tmp
    s3.download_file(
        'smartliva-models',
        'maxvit_large_best.pth',
        '/tmp/maxvit_large_best.pth'
    )
    
    s3.download_file(
        'smartliva-models',
        'medgemma_model.pth',
        '/tmp/medgemma_model.pth'
    )
    
    # Load models
    maxvit_model = torch.load('/tmp/maxvit_large_best.pth')
    medgemma_model = torch.load('/tmp/medgemma_model.pth')
```

### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 3: Google Cloud Storage

```bash
# Install gcloud
brew install google-cloud-sdk

# Login
gcloud auth login

# Create bucket
gsutil mb gs://smartliva-models

# Upload models
gsutil cp medgemma_model.pth gs://smartliva-models/
gsutil cp maxvit_large_best.pth gs://smartliva-models/

# Make public (optional)
gsutil iam ch allUsers:objectViewer gs://smartliva-models
```

### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 4: Git LFS (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å)

```bash
# Install Git LFS
brew install git-lfs
git lfs install

# Track large files
git lfs track "*.pth"
git add .gitattributes

# Commit
git add maxvit_large_best.pth medgemma_model.pth
git commit -m "Add models with LFS"
git push

# Note: GitHub LFS ‡∏°‡∏µ limit 1GB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö free tier
```

---

## üéØ ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Development (Local)

```bash
# ‡πÄ‡∏Å‡πá‡∏ö models ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
mkdir -p ~/SmartLiva-Models
mv medgemma_model.pth ~/SmartLiva-Models/
mv maxvit_large_best.pth ~/SmartLiva-Models/

# Symlink ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤ (optional)
ln -s ~/SmartLiva-Models/medgemma_model.pth .
ln -s ~/SmartLiva-Models/maxvit_large_best.pth .
```

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Production

1. **Upload models** ‚Üí Hugging Face / S3 / GCS
2. **‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î** ‚Üí Download on startup ‡∏´‡∏£‡∏∑‡∏≠ lazy loading
3. **Cache models** ‚Üí ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô `/tmp` ‡∏´‡∏£‡∏∑‡∏≠ persistent volume
4. **Optimize models**:
   - Model quantization (reduce size)
   - Model pruning
   - ONNX conversion

---

## üîß Model Optimization (‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î)

### Quantization (INT8)

```python
import torch

# Load original model
model = torch.load('medgemma_model.pth')

# Quantize to INT8 (reduce size by ~75%)
model_int8 = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# Save quantized model
torch.save(model_int8, 'medgemma_model_int8.pth')

# Before: 16GB ‚Üí After: ~4GB
```

### Model Pruning

```python
import torch.nn.utils.prune as prune

# Prune 30% of weights
for module in model.modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.3)

# Remove pruning reparameterization
for module in model.modules():
    if isinstance(module, torch.nn.Linear):
        prune.remove(module, 'weight')

torch.save(model, 'medgemma_model_pruned.pth')
```

### ONNX Conversion

```python
import torch.onnx

# Export to ONNX
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    opset_version=11
)
```

---

## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å

| Method | Cost | Size Limit | Speed | Best For |
|--------|------|------------|-------|----------|
| **Hugging Face** | ‡∏ü‡∏£‡∏µ | ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á | Open source |
| **AWS S3** | ~$0.023/GB/mo | ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î | ‡πÄ‡∏£‡πá‡∏ß | Production |
| **Google Cloud** | ~$0.020/GB/mo | ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î | ‡πÄ‡∏£‡πá‡∏ß | Production |
| **Git LFS** | ‡∏à‡∏≥‡∏Å‡∏±‡∏î (1GB free) | ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö plan | ‡∏ä‡πâ‡∏≤ | ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å |

---

## ‚úÖ Checklist

- [ ] Backup models ‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö
- [ ] ‡∏£‡∏±‡∏ô `./cleanup-large-files.sh`
- [ ] Upload models ‡πÑ‡∏õ cloud storage
- [ ] ‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ download models
- [ ] Test ‡πÉ‡∏ô local
- [ ] ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï `.gitignore`
- [ ] Commit ‡πÅ‡∏•‡∏∞ push
- [ ] Deploy!

---

## üÜò ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢

### Q: ‡∏ñ‡πâ‡∏≤‡∏•‡∏ö model files ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?

A: ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô local ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞ download ‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å cloud

### Q: Training data ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÑ‡∏´‡∏°?

A: ‡∏Ñ‡∏ß‡∏£ backup ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô (external drive, cloud) ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Git

### Q: node_modules ‡∏•‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏¢‡∏±‡∏á‡πÑ‡∏á?

A: ‡∏£‡∏±‡∏ô `cd frontend && npm install` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà

### Q: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö model files?

A: ‡πÉ‡∏ä‡πâ `.dockerignore` ‡πÅ‡∏•‡∏∞ `.vercelignore` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ deploy ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ ‡πÅ‡∏ï‡πà‡∏Å‡πá‡∏¢‡∏±‡∏á‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏ô local

---

## üìù ‡∏™‡∏£‡∏∏‡∏õ

**‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:** 48GB
**‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:** ~100MB ‚úÖ

**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:**
1. ‡∏£‡∏±‡∏ô `./cleanup-large-files.sh`
2. Upload models ‡πÑ‡∏õ Hugging Face (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
3. ‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ download ‡∏à‡∏≤‡∏Å cloud
4. Commit ‡πÅ‡∏•‡∏∞ deploy!

---

**üéâ ‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß deploy ‡∏à‡∏∞‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå!**
