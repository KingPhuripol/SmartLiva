# ü§ñ Migration: ‡∏à‡∏≤‡∏Å MedGemma (16GB) ‡πÑ‡∏õ GPT-4o API

## üéØ ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô?

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏î‡∏¥‡∏°:
- ‚ùå **medgemma_model.pth** - 16GB (‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å!)
- ‚ùå **data/** - 15GB training images
- ‚ùå Deploy ‡∏¢‡∏≤‡∏Å (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ GPU, RAM ‡∏™‡∏π‡∏á)
- ‚ùå Maintenance ‡∏¢‡∏≤‡∏Å
- ‚ùå Update model ‡∏¢‡∏≤‡∏Å

### ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á GPT-4o API:
- ‚úÖ **‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö model local** (0 bytes!)
- ‚úÖ **Deploy ‡∏á‡πà‡∏≤‡∏¢** ‡∏ö‡∏ô Vercel, Railway, Render
- ‚úÖ **Always up-to-date** - OpenAI update ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- ‚úÖ **Scalable** - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏á‡∏ß‡∏•‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á infrastructure
- ‚úÖ **Pay-per-use** - ‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
- ‚úÖ **GPT-4o ‡πÄ‡∏Å‡πà‡∏á‡∏Å‡∏ß‡πà‡∏≤ MedGemma** ‡∏°‡∏≤‡∏Å

---

## üöÄ ‡∏ß‡∏¥‡∏ò‡∏µ Migration (3 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô)

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå Migration

```bash
./migrate-to-gpt.sh
```

‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏à‡∏∞:
1. Backup model files ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà `~/Backup/SmartLiva-{timestamp}/`
2. ‡∏•‡∏ö medgemma_model.pth (16GB)
3. ‡∏•‡∏ö data/ (15GB)
4. ‡∏•‡∏ö node_modules (reinstall ‡πÑ‡∏î‡πâ)
5. Clean cache files

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: 48GB ‚Üí ~100MB** ‚úÖ

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OpenAI API Key

#### 2.1 ‡∏£‡∏±‡∏ö API Key ‡∏à‡∏≤‡∏Å OpenAI

1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
2. ‡∏™‡∏£‡πâ‡∏≤‡∏á account (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
3. ‡∏Ñ‡∏•‡∏¥‡∏Å "Create new secret key"
4. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å API key (‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ `sk-...`)

#### 2.2 ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Key

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Local Development:**

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á .env file
cat > backend/.env << EOF
OPENAI_API_KEY=sk-your-actual-api-key-here
OPENAI_MODEL=gpt-4o
EOF
```

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Production (Vercel/Railway/Render):**

‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment Variables ‡πÉ‡∏ô dashboard:
```
OPENAI_API_KEY=sk-your-actual-api-key-here
OPENAI_MODEL=gpt-4o
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: Test ‡πÅ‡∏•‡∏∞ Deploy

```bash
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
cd frontend && npm install
cd ../backend && pip install -r requirements.txt

# Test backend
cd backend
uvicorn app.main:app --reload

# Test ‡∏ó‡∏µ‡πà http://localhost:8000/docs

# ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‚Üí Commit ‡πÅ‡∏•‡∏∞ Deploy
git add .
git commit -m "Migrate to GPT-4o API, remove large local models"
git push origin main
vercel --prod
```

---

## üí∞ ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ GPT-4o

### GPT-4o Pricing (‡∏ì ‡∏û.‡∏¢. 2025)

| Model | Input | Output | Best For |
|-------|-------|--------|----------|
| **gpt-4o** | $2.50/1M tokens | $10.00/1M tokens | Production, high quality |
| **gpt-4o-mini** | $0.15/1M tokens | $0.60/1M tokens | Development, testing |

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:

**Scenario 1: Development/Testing**
- Model: gpt-4o-mini
- ‡πÉ‡∏ä‡πâ 100 requests/day
- Average 500 tokens/request
- ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢: ~$0.30/month üí∞

**Scenario 2: Production (Small)**
- Model: gpt-4o
- ‡πÉ‡∏ä‡πâ 1,000 requests/day
- Average 800 tokens/request
- ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢: ~$100/month üí∞üí∞

**Scenario 3: Production (Medium)**
- Model: gpt-4o
- ‡πÉ‡∏ä‡πâ 10,000 requests/day
- Average 800 tokens/request
- ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢: ~$1,000/month üí∞üí∞üí∞

### üí° Tips ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢:

1. **‡πÉ‡∏ä‡πâ gpt-4o-mini ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö development**
2. **Cache responses** ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
3. **Limit token usage** (max_tokens parameter)
4. **Implement rate limiting**
5. **Monitor usage** ‡πÉ‡∏ô OpenAI dashboard

---

## üîß Configuration Options

### Environment Variables

```bash
# Required
OPENAI_API_KEY=sk-xxx              # API key ‡∏à‡∏≤‡∏Å OpenAI

# Optional
OPENAI_MODEL=gpt-4o                # Model: gpt-4o, gpt-4o-mini, gpt-4-turbo
MAX_TOKENS=300                      # Max tokens per response
TEMPERATURE=0.7                     # Creativity (0.0-2.0)
```

### Model Choices

```python
# backend/app/main.py

# Option 1: GPT-4o (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production)
OPENAI_MODEL=gpt-4o

# Option 2: GPT-4o-mini (‡∏ñ‡∏π‡∏Å‡∏Å‡∏ß‡πà‡∏≤, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dev)
OPENAI_MODEL=gpt-4o-mini

# Option 3: GPT-4 Turbo (balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á cost & quality)
OPENAI_MODEL=gpt-4-turbo
```

---

## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: MedGemma vs GPT-4o

| Feature | MedGemma (‡πÄ‡∏î‡∏¥‡∏°) | GPT-4o (‡πÉ‡∏´‡∏°‡πà) |
|---------|-----------------|----------------|
| **‡∏Ç‡∏ô‡∏≤‡∏î** | 16GB | 0 bytes |
| **Deploy** | ‡∏¢‡∏≤‡∏Å (‡∏ï‡πâ‡∏≠‡∏á GPU) | ‡∏á‡πà‡∏≤‡∏¢ |
| **Cost** | Free ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏á | Pay-per-use |
| **Performance** | ‡∏î‡∏µ | ‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° |
| **Updates** | Manual | Auto by OpenAI |
| **Scalability** | ‡∏à‡∏≥‡∏Å‡∏±‡∏î | Unlimited |
| **Maintenance** | ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡πÅ‡∏•‡πÄ‡∏≠‡∏á | OpenAI ‡∏î‡∏π‡πÅ‡∏•‡πÉ‡∏´‡πâ |

---

## üîê Security Best Practices

### 1. ‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á API Key

```bash
# ‚ùå ‡∏≠‡∏¢‡πà‡∏≤‡∏ó‡∏≥ - commit API key ‡πÉ‡∏ô code
OPENAI_API_KEY="sk-xxx"

# ‚úÖ ‡∏ó‡∏≥ - ‡πÉ‡∏ä‡πâ environment variables
OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")
```

### 2. Rate Limiting

```python
from fastapi import HTTPException
import time

# Simple rate limiter
user_requests = {}

def check_rate_limit(user_id: str, max_requests: int = 100, window: int = 3600):
    now = time.time()
    if user_id not in user_requests:
        user_requests[user_id] = []
    
    # Remove old requests
    user_requests[user_id] = [t for t in user_requests[user_id] if now - t < window]
    
    if len(user_requests[user_id]) >= max_requests:
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    
    user_requests[user_id].append(now)
```

### 3. Input Validation

```python
def validate_chat_input(message: str) -> bool:
    if len(message) > 2000:  # Max message length
        return False
    if not message.strip():  # Empty message
        return False
    return True
```

---

## üß™ Testing

### Local Testing

```bash
# Start backend
cd backend
uvicorn app.main:app --reload --port 8000

# Test chat endpoint
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "history": [
      {"role": "user", "content": "What is liver fibrosis?"}
    ],
    "max_new_tokens": 300,
    "temperature": 0.7
  }'
```

### Production Testing

```bash
# Test production API
curl -X POST https://your-api.vercel.app/chat \
  -H "Content-Type: application/json" \
  -d '{
    "history": [
      {"role": "user", "content": "Explain HCC"}
    ]
  }'
```

---

## üìà Monitoring

### 1. OpenAI Dashboard
- ‡πÑ‡∏õ‡∏ó‡∏µ‡πà [platform.openai.com/usage](https://platform.openai.com/usage)
- ‡∏î‡∏π token usage, costs, requests

### 2. Backend Logs

```python
import logging

logger = logging.getLogger(__name__)

@app.post("/chat")
async def chat(request: ChatRequest):
    logger.info(f"Chat request: {len(request.history)} messages")
    
    result = try_openai_chat(...)
    
    if result:
        reply, tokens = result
        logger.info(f"Response: {tokens} tokens used")
    
    return response
```

---

## üÜò Troubleshooting

### ‚ùå "Invalid API Key"

**Solution:**
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API key
echo $OPENAI_API_KEY

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà
export OPENAI_API_KEY=sk-your-actual-key
```

### ‚ùå "Rate limit exceeded"

**Solution:**
- Upgrade OpenAI plan
- Implement caching
- Use gpt-4o-mini

### ‚ùå "Context length exceeded"

**Solution:**
```python
# Limit history length
history = history[-5:]  # Keep only last 5 messages

# Reduce max_tokens
max_tokens = 300  # Instead of 1000
```

---

## ‚úÖ Checklist

- [ ] ‡∏£‡∏±‡∏ô `./migrate-to-gpt.sh`
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á OpenAI account ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö API key
- [ ] ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ `OPENAI_API_KEY` ‡πÉ‡∏ô .env
- [ ] Test local: `uvicorn app.main:app --reload`
- [ ] Test chat endpoint
- [ ] ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment Variables ‡πÉ‡∏ô Vercel/Railway
- [ ] Deploy: `vercel --prod`
- [ ] Monitor usage ‡πÉ‡∏ô OpenAI dashboard
- [ ] Set up rate limiting (optional)
- [ ] Set up caching (optional)

---

## üéâ ‡∏™‡∏£‡∏∏‡∏õ

**‡∏Å‡πà‡∏≠‡∏ô:**
- ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå 48GB
- Deploy ‡∏¢‡∏≤‡∏Å
- ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ model ‡πÄ‡∏≠‡∏á

**‡∏´‡∏•‡∏±‡∏á:**
- ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå ~100MB ‚úÖ
- Deploy ‡∏á‡πà‡∏≤‡∏¢ ‚úÖ
- OpenAI ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ ‚úÖ
- GPT-4o ‡πÄ‡∏Å‡πà‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‚úÖ

**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:**
```bash
./migrate-to-gpt.sh
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OPENAI_API_KEY
vercel --prod
```

---

**‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°?** ‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢! üòä

**‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ cost-effective?** ‡πÉ‡∏ä‡πâ `gpt-4o-mini` ‡πÅ‡∏ó‡∏ô `gpt-4o`
